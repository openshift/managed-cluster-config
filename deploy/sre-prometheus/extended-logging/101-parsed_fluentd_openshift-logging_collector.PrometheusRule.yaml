# taken from f029cca0-fd96-4cde-afa4-77c151c5233d
# using oc get prometheusrule -n openshift-logging collector -oyaml
# also:
# oc get csv -n openshift-logging | grep -i -e elastic -elogging
# cluster-logging.5.3.5-20        Red Hat OpenShift Logging          5.3.5-20 Succeeded
# elasticsearch-operator.5.3.5-20 OpenShift Elasticsearch Operator   5.3.5-20 Succeeded
# and
# https://catalog.redhat.com/software/containers/openshift-logging/cluster-logging-operator-bundle/5fd22f465d2ec16f0da1e8c8
# where the current latest version is:
# ```
# $ date
# Sun May  1 16:04:22 IDT 2022
# $ echo "the version is v5.4.0-138" # there is also 5.4.6-46
# ...
# ```
# created this file via
# ```
# A=$(curl -sSLo- https://github.com/openshift/cluster-logging-operator/raw/b26497cce95e7737a4b66b5ff6b822b80a16bc3a/files/fluentd/fluentd_prometheus_alerts.yaml) yq '.spec = env(A) | .spec.groups[].rules[].alert += "SRE" | .spec.groups[].rules[].labels.namespace = "openshift-logging" | .metadata.name += "-sre"' ../../../resources/prometheusrules/fluentd_openshift-logging_collector.PrometheusRule.yaml | yq -P > 101-parsed_fluentd_openshift-logging_collector.PrometheusRule.yaml
# ```
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector-sre
  namespace: openshift-logging
spec:
  groups:
    - name: logging_fluentd.alerts
      rules:
        - alert: FluentdNodeDownSRE
          annotations:
            message: Prometheus could not scrape fluentd or vector pods {{ $labels.instance }} for more than 10m.
            summary: Fluentd cannot be scraped
          expr: |
            absent(up{job="collector"} == 1) and absent(up{job="fluentd"} == 1) and absent(up{app_kubernetes_io_component="collector"}==1)
          for: 10m
          labels:
            service: fluentd
            severity: critical
            namespace: openshift-logging
        - alert: FluentdQueueLengthIncreasingSRE
          annotations:
            message: For the last hour, fluentd {{ $labels.instance }} average buffer queue length has increased continuously.
            summary: Fluentd unable to keep up with traffic over time.
          expr: |
            ( 0 * (kube_pod_start_time{pod=~".*fluentd.*"} < time() - 3600 ) )  + on(pod)  label_replace( ( deriv(fluentd_output_status_buffer_queue_length[10m]) > 0 and delta(fluentd_output_status_buffer_queue_length[1h]) > 1 ), "pod", "$1", "hostname", "(.*)")
          for: 1h
          labels:
            service: fluentd
            severity: error
            namespace: openshift-logging
        - alert: VectorDiskBufferUsageSRE
          annotations:
            description: 'Grandfathered openshift-logging collectors potentially consuming too much node disk, {{ $value
              }}% '
            summary: Detected consuming too much node disk on $labels.hostname host
          expr: "(label_replace(sum by(hostname) (vector_buffer_byte_size{component_kind='sink',
            buffer_type='disk'}), 'instance', '$1', 'hostname', '(.*)') \n/ on(instance)
            group_left() sum by(instance) (node_filesystem_size_bytes{mountpoint='/var'}))
            * 100  > 15\n"
          for: 5m
          labels:
            service: collector
            severity: Warning
        - alert: FluentDHighErrorRateSRE
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd {{ $labels.instance }}.'
            summary: FluentD output errors are high
          expr: |
            100 * (
              sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
            /
              sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
            ) > 10
          for: 15m
          labels:
            severity: warning
            namespace: openshift-logging
        - alert: FluentDVeryHighErrorRateSRE
          annotations:
            message: '{{ $value }}% of records have resulted in an error by fluentd {{ $labels.instance }}.'
            summary: FluentD output errors are very high
          expr: |
            100 * (
              sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
            /
              sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
            ) > 25
          for: 15m
          labels:
            severity: critical
            namespace: openshift-logging
        - alert: CollectorHigh403ForbiddenResponseRateSRE
          annotations:
            description: Grandfathered openshift-logging high rate of "HTTP 403 Forbidden" responses detected for collector
              "{{ $labels.app_kubernetes_io_instance }}" in namespace {{ $labels.namespace
              }} for output "{{ $labels.component_id }}". The rate of 403 responses is
              {{ printf "%.2f" $value }}% over the last 2 minutes, persisting for more
              than 5 minutes. This could indicate an authorization issue.
            runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-logging-operator/CollectorHigh403ForbiddenResponseRate.md
            summary: At least 10% of sent requests responded with "HTTP 403 Forbidden"
              for collector "{{ $labels.app_kubernetes_io_instance }}" in namespace {{
              $labels.namespace }} for output "{{ $labels.component_id }}"
          expr: |
            sum(
              irate(vector_http_client_responses_total{component_kind="sink", status="403"}[2m])
            ) by (component_id, app_kubernetes_io_instance, namespace)
            /
            sum(
              irate(vector_http_client_responses_total{component_kind="sink"}[2m])
            ) by (component_id, app_kubernetes_io_instance, namespace)
            * 100
            > 10
          for: 5m
          labels:
            service: collector
            severity: critical
