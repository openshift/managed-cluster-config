apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-control-plane-resizing-alerts
    role: alert-rules
  name: sre-control-plane-resizing-alerts
  namespace: openshift-monitoring
spec:
  # https://docs.openshift.com/rosa/rosa_planning/rosa-limits-scalability.html#control-plane-and-infra-node-sizing-and-scaling-sd_rosa-limits-scalability
  groups:
  - name: sre-control-plane-resizing-recording.rules
    rules:
      - expr: label_replace(cluster:nodes_roles, "instance", "$1", "node", "(.*)") * on(instance) group_left node_memory_MemTotal_bytes
        record: sre:node_roles:memory_total_bytes
      ## Expression Explanation:
      ## Average of value of
      ## the Average (per node) rate of change of CPU time spent in non-idle modes, totalled by CPU, looking back across the past 8h,
      ## "*" applies a label replace to limit output to control plane nodes
      ## Greater than (>) the threshold which is (n-1)/n where n is the number of control plane nodes, evaluating to 2/3 in most circumstances.
      - expr: (
                avg (
                  avg by (instance) (
                    sum by (cpu, instance) (
                      rate(
                        node_cpu_seconds_total{mode!="idle"}[8h]
                      )
                    )
                  )
                  *
                  on (instance) (
                    label_replace (
                      kube_node_role{role ="master"}, "instance", "$1", "node", "(.*)"
                    )
                  )
                )
                >
                (
                  scalar (
                    (
                      count (
                        cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                      )
                      - 1
                    )
                    /
                    count (
                      cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                    )
                  )
                )
              )
        record: sre:node_control_plane:excessive_consumption_cpu
      ## Expression Explanation:
      ## 1, minus the total amount of free memory divided by the total amount of memory for the infra node type, gives us the percent used memory as a decimal value: 0.%%
      ## Greater than (>) the threshold which is (n-1)/n where n is the number of control plane nodes, evaluating to 2/3 in most circumstances.
      - expr: ( 1 -
                sum (
                    node_memory_MemFree_bytes +
                    node_memory_Buffers_bytes +
                    node_memory_Cached_bytes
                    AND on (instance) label_replace(
                        kube_node_role{role="master"}, "instance", "$1", "node", "(.+)"
                    )
                )
                /
                sum (
                    node_memory_MemTotal_bytes
                    AND on (instance) label_replace(
                        kube_node_role{role="master"}, "instance", "$1", "node", "(.+)"
                    )
                )
            )
            >
            (
              scalar (
                (
                  count (
                    cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                  )
                  - 1
                )
                /
                count (
                  cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                )
              )
            )
        record: sre:node_control_plane:excessive_consumption_memory
      ## CPU: % usage = 100 × non-idle/total; >80% flagged as 1 per 5m sample.
      ## avg_over_time gives fraction of last 24h masters were >80% CPU.
      ## count_over_time ensures ≥12h of data before emitting values.
      - expr: avg_over_time (
                (
                  (
                    100 * sum by (instance) (
                      rate( node_cpu_seconds_total{mode!="idle"}[5m] )
                    )
                    /
                    sum by (instance) (
                      rate( node_cpu_seconds_total[5m] )
                    )
                    AND on(instance) label_replace(
                      kube_node_role{role="master"},"instance", "$1", "node", "(.+)"
                    )
                  ) > bool 80
                )[24h:5m]
              ) AND count_over_time(
                (
                  (
                    100 * sum by (instance) (
                      rate( node_cpu_seconds_total{mode!="idle"}[5m] )
                    )
                    /
                    sum by (instance) (
                      rate( node_cpu_seconds_total[5m] )
                    )
                    AND on(instance) label_replace(
                      kube_node_role{role="master"},"instance", "$1", "node", "(.+)"
                    )
                  )
                )[24h:5m]
              ) > (12 * 60/5)
        record: sre:node_control_plane:cpu_usage_ebb
      ## Memory: % used = (total − (free+buffers+cached)) / total × 100; >80% flagged as 1.
      ## avg_over_time gives fraction of last 24h masters were >80% memory.
      ## count_over_time ensures ≥12h of data before emitting values.
      - expr: avg_over_time (
                (
                  (
                    (
                      (
                        ( node_memory_MemTotal_bytes
                          - ( node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes )
                        )
                        / node_memory_MemTotal_bytes
                      ) * 100
                    )
                    AND on(instance) label_replace(
                      kube_node_role{role="master"}, "instance", "$1", "node", "(.+)"
                    )
                  ) > bool 80
                )[24h:5m]
              ) AND count_over_time(
                (
                  (
                    ( node_memory_MemTotal_bytes
                      - ( node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes )
                    )
                    / node_memory_MemTotal_bytes
                  ) * 100
                  AND on(instance) label_replace(
                    kube_node_role{role="master"},"instance", "$1", "node", "(.+)"
                  )
                )[24h:5m]
              ) > (12 * 60/5)
        record: sre:node_control_plane:memory_usage_ebb
  - name: sre-control-plane-resizing-alerts
    rules:
        # This used to be called MasterNodesNeedResizingSRE
      - alert: ControlPlaneNodesNeedResizingSRE
        expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
        for: 15m
        labels:
          severity: warning
          namespace: openshift-monitoring
        annotations:
          message: "The cluster's control plane nodes have been undersized for 15 minutes and should be vertically scaled to support the existing workload. See linked SOP for details. Critical alert will be raised at 24 hours."
        # This used to be called MasterNodesNeedResizingSRE
      - alert: ControlPlaneNodesNeedResizingSRE
        expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
        for: 24h
        labels:
          severity: critical
          namespace: openshift-monitoring
        annotations:
          message: "The cluster's control plane nodes have been undersized for 24 hours and must be vertically scaled to support the existing workload. See linked SOP for details."
      # Alerts critical if control plane CPU or memory >80% for over 2h in the past 24h.
      - alert: ControlPlaneNodeErrorBudgetBurn
        expr: (sre:node_control_plane:cpu_usage_ebb > (2/24)) or (sre:node_control_plane:memory_usage_ebb > (2/24))
        for: 5m
        labels:
          severity: warning
          namespace: openshift-monitoring
        annotations:
          description: "Control plane CPU or memory usage exceeded 80% for >2h within the last 24h. Consider resizing nodes. See linked SOP for details."
          runbook_url: https://github.com/openshift/ops-sop/blob/master/v4/alerts/ControlPlaneNodeErrorBudgetBurn.md
