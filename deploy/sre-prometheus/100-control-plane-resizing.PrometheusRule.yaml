apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-control-plane-resizing-alerts
    role: alert-rules
  name: sre-control-plane-resizing-alerts
  namespace: openshift-monitoring
spec:
  # https://docs.openshift.com/rosa/rosa_planning/rosa-limits-scalability.html#control-plane-and-infra-node-sizing-and-scaling-sd_rosa-limits-scalability
  groups:
  - name: sre-control-plane-resizing-recording.rules
    rules:
      - expr: label_replace(cluster:nodes_roles, "instance", "$1", "node", "(.*)") * on(instance) group_left node_memory_MemTotal_bytes
        record: sre:node_roles:memory_total_bytes
      ## Expression Explanation:
      ## Average of value of
      ## the Average (per node) rate of change of CPU time spent in non-idle modes, totalled by CPU, looking back across the past 8h,
      ## "*" applies a label replace to limit output to control plane nodes
      ## Greater than (>) the threshold which is (n-1)/n where n is the number of control plane nodes, evaluating to 2/3 in most circumstances.
      - expr: ( 
                avg (
                  avg by (instance) (
                    sum by (cpu, instance) (
                      rate(
                        node_cpu_seconds_total{mode!="idle"}[8h]
                      )
                    )
                  )
                  *
                  on (instance) (
                    label_replace (
                      kube_node_role{role ="master"}, "instance", "$1", "node", "(.*)"
                    )
                  )
                )
                >
                (
                  scalar (
                    (
                      count (
                        cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                      )
                      - 1
                    )
                    /
                    count (
                      cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                    )
                  )
                )
              )
        record: sre:node_control_plane:excessive_consumption_cpu
      ## Expression Explanation: 
      ## 1, minus the total amount of free memory divided by the total amount of memory for the infra node type, gives us the percent used memory as a decimal value: 0.%%
      ## Greater than (>) the threshold which is (n-1)/n where n is the number of control plane nodes, evaluating to 2/3 in most circumstances.
      - expr: ( 1 -
                sum (
                    node_memory_MemFree_bytes +
                    node_memory_Buffers_bytes +
                    node_memory_Cached_bytes
                    AND on (instance) label_replace(
                        kube_node_role{role="master"}, "instance", "$1", "node", "(.+)"
                    )
                ) 
                / 
                sum (
                    node_memory_MemTotal_bytes
                    AND on (instance) label_replace(
                        kube_node_role{role="master"}, "instance", "$1", "node", "(.+)"
                    )
                )
            ) 
            > 
            (
              scalar (
                (
                  count (
                    cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                  )
                  - 1
                )
                /
                count (
                  cluster:nodes_roles{label_node_role_kubernetes_io ="master"}
                )
              )
            )
        record: sre:node_control_plane:excessive_consumption_memory
      ## Calculates CPU usage per master node as 100 – idle%.  
      ## Converts each sample to 1 if usage >80%, 0 otherwise.  
      ## avg_over_time(...[24h:5m]) computes the fraction of the last 24h each node was above 80% CPU.                 
      - expr: |
          avg_over_time(
            (
              (
                (1 - avg by (instance) (
                  rate(node_cpu_seconds_total{mode="idle"}[5m])
                )) * 100
                AND on(instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
              ) > bool 80
            )[24h:5m]
          )
        record: sre:node_control_plane:cpu_usage_ebb
      ## Calculates memory usage per master node as (1 – free/total) × 100.  
      ## Converts each sample to 1 if usage >80%, 0 otherwise.  
      ## avg_over_time(...[24h:5m]) computes the fraction of the last 24h each node was above 80% memory.
      - expr: |
          avg_over_time(
            (
              (
                1 - (
                  (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)
                  / node_memory_MemTotal_bytes
                )
              ) * 100 > bool 80
              AND on(instance) label_replace(kube_node_role{role="master"}, "instance", "$1", "node", "(.+)")
            )[24h:5m]
          )
        record: sre:node_control_plane:memory_usage_ebb
  - name: sre-control-plane-resizing-alerts
    rules:
        # This used to be called MasterNodesNeedResizingSRE
      - alert: ControlPlaneNodesNeedResizingSRE
        expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
        for: 15m
        labels:
          severity: warning
          namespace: openshift-monitoring
        annotations:
          description: "The cluster's control plane nodes have been undersized for 15 minutes and should be vertically scaled to support the existing workload. See linked SOP for details. Critical alert will be raised at 24 hours."
        # This used to be called MasterNodesNeedResizingSRE
      - alert: ControlPlaneNodesNeedResizingSRE
        expr: (sre:node_control_plane:excessive_consumption_memory > 0) or (sre:node_control_plane:excessive_consumption_cpu > 0)
        for: 24h
        labels:
          severity: critical
          namespace: openshift-monitoring
        annotations:
          description: "The cluster's control plane nodes have been undersized for 24 hours and must be vertically scaled to support the existing workload. See linked SOP for details."

      - alert: ControlPlaneNodeErrorBudgetBurn
        expr: (sre:node_control_plane:cpu_usage_ebb > (2/24)) or (sre:node_control_plane:memory_usage_ebb > (2/24))
        for: 5m
        labels:
          severity: critical
          namespace: openshift-monitoring
        annotations:
          description: "Control plane CPU or memory usage exceeded 80% for >2h within the last 24h. Consider resizing nodes. See linked SOP for details."
          runbook_url: "https://github.com/openshift/ops-sop/blob/master/v4/alerts/ControlPlaneNodeErrorBudgetBurn.md"
