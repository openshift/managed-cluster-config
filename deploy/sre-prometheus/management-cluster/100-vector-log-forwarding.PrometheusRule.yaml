apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sre-vector-log-forwarding
  namespace: openshift-monitoring
  labels:
    prometheus: sre-vector-log-forwarding
    role: alert-rules
spec:
  groups:
  - name: sre-vector-log-forwarding-recording-rules
    rules:

    # ============================================================================
    # RECORDING RULES - Pre-calculated metrics for dashboards and alerts
    # ============================================================================

    # Calculates the ratio of logs successfully written to S3 vs logs collected from HCP pods
    # Used for SLO tracking and error budget burn calculations
    # Value should be ~1.0 (100% delivery success)
    # Values >1.0 (e.g., 1.15) are normal when draining buffered logs or rate window variance
    # Acceptable: 1.0-1.3 normal, <1.0 sustained indicates backup, >1.5 sustained warrants investigation
    - record: vector:log_delivery:success_ratio
      expr: |
        sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m]))
        /
        sum(rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m]))

    # Same as above but broken down per HCP cluster namespace
    # Used for per-cluster error budget burn alerts
    # Value should be ~1.0 (100% delivery success per cluster)
    - record: vector:log_delivery:success_ratio:by_cluster
      expr: |
        sum by (pod_namespace) (
          rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m])
        )
        /
        sum by (pod_namespace) (
          rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])
        )

    # Pipeline processing ratio per HCP cluster namespace
    # Measures pipeline processing ratio per cluster (logs through transforms / logs received)
    # NOTE: This uses custom metric created BEFORE S3 sink, so it measures transform pipeline health, not S3 delivery success
    # Value should be ~1.0 (logs making it through transforms)
    - record: vector:log_delivery:pipeline_ratio:by_cluster
      expr: |
        sum by (pod_namespace) (
          rate(vector_hcp_logs_events_total[5m])
        )
        /
        sum by (pod_namespace) (
          rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])
        )

    # Measures how well the S3 sink is keeping up with incoming logs
    # Compares logs queued for S3 vs logs successfully written
    # Value should be ~1.0 (S3 writes keeping up with queue)
    # Values <1.0 indicate backpressure (queue filling faster than S3 writes, buffer will grow)
    # Values >1.0 indicate S3 draining the queue (catching up from previous backlog)
    - record: vector:s3_sink:write_ratio
      expr: |
        sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m]))
        /
        sum(rate(vector_component_received_events_total{component_id="hcp_logs"}[5m]))

    # Total log ingestion rate across all HCP clusters
    # Useful for capacity planning and baseline metrics
    # Value represents events/sec collected from all HCP pod logs
    - record: vector:logs:ingestion_rate
      expr: |
        sum(rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m]))

    # Total S3 delivery rate
    # Useful for capacity planning and baseline metrics
    # Value represents events/sec successfully written to S3
    # Should be close to ingestion_rate (within ~15%) in steady state
    - record: vector:logs:s3_delivery_rate
      expr: |
        sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m]))

    # Per-pod log ingestion rate
    # Identifies which Vector pods are receiving high log volume from their assigned HCP clusters
    # Useful for diagnosing buffer capacity issues and identifying noisy clusters
    - record: vector:logs:ingestion_rate:by_pod
      expr: |
        sum by (pod) (
          rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])
        )

    # Per-pod S3 delivery rate
    # Identifies if specific Vector pods have S3 write performance issues
    # Compare to ingestion_rate:by_pod to identify backpressure on specific pods
    - record: vector:logs:s3_delivery_rate:by_pod
      expr: |
        sum by (pod) (
          rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m])
        )

    # Per-pod write ratio (S3 delivery / ingestion)
    # <1.0 = ingestion exceeds delivery (buffer growing - high log volume or S3 slow)
    # ~1.0 = balanced (healthy steady state)
    # >1.0 = delivery exceeds ingestion (buffer draining - catching up)
    - record: vector:logs:write_ratio:by_pod
      expr: |
        sum by (pod) (
          rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m])
        )
        /
        sum by (pod) (
          rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])
        )

  - name: sre-vector-log-forwarding-critical-alerts
    rules:

    # ============================================================================
    # CRITICAL ALERTS - Immediate attention required
    # NOTE: Temporarily set to severity: warning for initial deployment and validation
    #       These will be changed to severity: critical after soaking period
    # ============================================================================

    # CRITICAL: Vector has stopped collecting logs from HCP pods entirely
    # This indicates a complete failure of log collection
    # Possible causes: Vector pods down, permissions issues, k8s API problems
    # TEMPORARILY WARNING: Set to warning for initial validation
    - alert: VectorNoLogsIngestedSRE
      expr: |
        sum(rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])) == 0
      for: 5m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: Vector is not collecting any logs from HCP pods
        description: No logs have been collected in the last 5 minutes. Check Vector pod status and k8s API connectivity.
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorNoLogsIngestedSRE.md

    # CRITICAL: Vector has stopped writing logs to S3 entirely
    # Logs are being collected but not delivered - they will be buffered until disk fills
    # Possible causes: S3 connectivity issues, IAM role problems, S3 bucket access denied
    # TEMPORARILY WARNING: Set to warning for initial validation
    - alert: VectorS3WritesStoppedSRE
      expr: |
        sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m])) == 0
      for: 5m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: Vector has stopped writing logs to S3
        description: No logs have been written to S3 in the last 5 minutes. Buffer will eventually fill. Check S3 access and IAM roles.
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorS3WritesStoppedSRE.md

    # CRITICAL: Burning through error budget extremely fast (per HCP cluster)
    # Threshold: 0.0144 = 1.44% failure rate (logs not delivered to S3)
    # Error budget calculation: Assuming 99.9% SLO over 30 days (720 hours)
    #   - Allowed error budget: 0.1% = 0.72 hours of failures over 30 days
    #   - At 1.44% failure rate sustained for 1 hour: uses 0.0144 hours of error budget
    #   - 0.0144 / 0.72 = 2% of entire 30-day error budget consumed in 1 hour
    #   - At this rate: would exhaust 100% of error budget in 50 hours (~2 days)
    # This indicates major delivery problems requiring immediate investigation
    # Fires per HCP cluster - pod_namespace label identifies affected cluster
    # TEMPORARILY WARNING: Set to warning for initial validation
    - alert: VectorErrorBudgetBurnFastSRE
      expr: |
        (1 - vector:log_delivery:success_ratio:by_cluster) > 0.0144
      for: 2m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "Vector is burning through error budget rapidly for cluster {{ $labels.pod_namespace }}"
        description: "HCP cluster {{ $labels.pod_namespace }} delivery failure rate: {{ $value | humanizePercentage }}. At current rate, would exhaust 5% of 30-day error budget in 1 hour."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorErrorBudgetBurnFastSRE.md

    # CRITICAL: Vector is missing MC_CLUSTER_ID environment variable
    # This indicates the ClusterDeployment label 'api.openshift.com/id' is missing
    # or Hive template interpolation failed during SelectorSyncSet deployment
    # Impact:
    #   - All logs will be written to S3 path: s3://bucket/unknown/...
    #   - Cannot identify which Management Cluster generated the logs
    #   - Multiple MCs writing to same bucket will have path collisions
    #   - Troubleshooting becomes extremely difficult
    # Resolution:
    #   - Verify ClusterDeployment has label: api.openshift.com/id
    #   - Check Hive SelectorSyncSet template interpolation
    #   - Restart Vector pods after fixing label
    # TEMPORARILY WARNING: Set to warning for initial validation
    - alert: VectorMissingClusterIDSRE
      expr: |
        sum by (mc_cluster_id) (
          rate(vector_hcp_logs_events_total{mc_cluster_id="unknown"}[5m])
        ) > 0
      for: 5m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: Vector is missing MC_CLUSTER_ID environment variable
        description: "Logs are being tagged with mc_cluster_id='unknown', indicating the ClusterDeployment label 'api.openshift.com/id' is missing or Hive template interpolation failed. S3 logs will be unidentifiable by Management Cluster."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorMissingClusterIDSRE.md

  - name: sre-vector-log-forwarding-warning-alerts
    rules:

    # ============================================================================
    # WARNING ALERTS - Investigation needed but not immediately critical
    # ============================================================================

    # WARNING: Disk buffer is >70% full (7GB of 10GB)
    # Buffer filling indicates S3 writes are slower than log ingestion
    # If buffer reaches 100%, Vector will BLOCK log collection (when_full: block)
    - alert: VectorBufferNearCapacitySRE
      expr: |
        vector_buffer_byte_size{buffer_id="hcp_logs"} > 7516192768
      for: 10m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "Vector S3 buffer is >70% full on pod {{ $labels.pod }}"
        description: "Buffer size: {{ $value | humanize1024 }}B (>7GB threshold). If buffer reaches 10GB, log collection will be blocked."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorBufferNearCapacitySRE.md

    # WARNING: S3 write operations are experiencing errors
    # Some logs may be retried successfully, but persistent errors indicate problems
    # Possible causes: Throttling, temporary connectivity issues, partial IAM permissions
    - alert: VectorS3ErrorsSRE
      expr: |
        sum(rate(vector_component_errors_total{component_id="hcp_logs"}[5m])) > 0
      for: 5m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: Vector is experiencing S3 write errors
        description: "S3 error rate: {{ $value | humanize }} errors/sec. Check Vector logs and S3 service status."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorS3ErrorsSRE.md

    # WARNING: S3 sink is not keeping up with incoming logs
    # Logs are being queued faster than they're being written to S3
    # Will eventually cause buffer to fill if not resolved
    - alert: VectorS3WriteBackpressureSRE
      expr: |
        (
          sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m]))
          /
          sum(rate(vector_component_received_events_total{component_id="hcp_logs"}[5m]))
        ) < 0.95
      for: 30m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: S3 sink is not keeping up with incoming logs
        description: "S3 write rate is {{ $value | humanizePercentage }} of queue rate. Buffer will grow over time. Check S3 write latency."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorS3WriteBackpressureSRE.md

    # WARNING: Logs are being dropped somewhere in the Vector pipeline
    # End-to-end delivery is less than what was collected from HCP pods
    # Possible causes: Transform errors, drops, or S3 sink issues
    - alert: VectorPipelineLossSRE
      expr: |
        (
          sum(rate(vector_component_sent_events_total{component_id="hcp_logs"}[5m]))
          /
          sum(rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m]))
        ) < 0.98
      for: 30m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: Logs are being lost in Vector pipeline
        description: "End-to-end delivery: {{ $value | humanizePercentage }}. Expected ~100%. Check for transform errors or S3 sink issues."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorPipelineLossSRE.md

    # WARNING: Burning through error budget slowly (per HCP cluster)
    # Threshold: 0.003 = 0.3% failure rate (logs not delivered to S3)
    # Error budget calculation: Assuming 99.9% SLO over 30 days (720 hours)
    #   - Allowed error budget: 0.1% = 0.72 hours of failures over 30 days
    #   - At 0.3% failure rate sustained for 6 hours: uses 0.018 hours of error budget
    #   - 0.018 / 0.72 = 2.5% of entire 30-day error budget consumed in 6 hours
    #   - At this rate: would exhaust 100% of error budget in 10 days
    # This indicates ongoing but manageable delivery issues that should be investigated
    # Fires per HCP cluster - pod_namespace label identifies affected cluster
    - alert: VectorErrorBudgetBurnSlowSRE
      expr: |
        (1 - vector:log_delivery:success_ratio:by_cluster) > 0.003
      for: 1h
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "Vector is burning through error budget (slow burn) for cluster {{ $labels.pod_namespace }}"
        description: "HCP cluster {{ $labels.pod_namespace }} delivery failure rate: {{ $value | humanizePercentage }}. At current rate, would exhaust 10% of 30-day error budget in 6 hours."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorErrorBudgetBurnSlowSRE.md

    # WARNING: Disk buffer is growing rapidly (>100MB/min)
    # Uses deriv() to calculate buffer growth RATE (bytes/sec change over 5 minutes)
    # Threshold: 1,677,721.6 bytes/sec = 100 MB/min = 6 GB/hour
    # IMPORTANT - Understanding deriv() values:
    #   - Positive values = buffer GROWING (e.g., 3,000,000 bytes/sec = 180 MB/min growth)
    #   - Higher values = faster growth (buffer filling rapidly)
    #   - Lower positive values = slower growth (buffer still growing, just slower)
    #   - Negative values = buffer SHRINKING (not shown in this alert, filtered out by > threshold)
    #   - Alert appears/disappears as growth rate crosses threshold
    # Early warning that S3 writes are falling behind - fires before VectorBufferNearCapacity
    # Could indicate S3 throttling, increasing log volume, or insufficient S3 write capacity
    - alert: VectorBufferGrowingRapidlySRE
      expr: |
        deriv(vector_buffer_byte_size{buffer_id="hcp_logs"}[5m]) > 1677721.6
      for: 10m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "Vector buffer growing rapidly on pod {{ $labels.pod }}"
        description: "Buffer growth rate: {{ $value | humanize }}B/sec (>100MB/min threshold). IMPORTANT: This value is the GROWTH RATE (how fast buffer is filling), not the buffer size."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorBufferGrowingRapidlySRE.md

    # WARNING: Errors detected in Vector transform components
    # Transform components process logs using VRL (Vector Remap Language) scripts to:
    #   - enrich_metadata: Extract namespace, pod, container info from kubernetes metadata
    #   - parse_json_logs: Parse JSON-formatted logs and extract timestamps
    #   - parse_plain_text_timestamps: Parse timestamps from plain text log messages
    # Errors in these transforms can cause:
    #   - Log drops (if transform cannot process log event)
    #   - Missing metadata (logs delivered without proper namespace/pod labels)
    #   - Incorrect timestamps (logs using Vector ingestion time instead of log time)
    # Common causes: Invalid log formats, missing expected fields, VRL script bugs
    - alert: VectorTransformErrorsSRE
      expr: |
        sum by (component_id) (
          rate(vector_component_errors_total{
            component_id=~"enrich_metadata|parse_json_logs|parse_plain_text_timestamps"
          }[5m])
        ) > 0
      for: 5m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "Vector transform {{ $labels.component_id }} is experiencing errors"
        description: "Transform error rate: {{ $value | humanize }} errors/sec. Logs may be dropped or delivered with incomplete metadata."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorTransformErrorsSRE.md

    # WARNING: A specific HCP cluster is generating excessive logs
    # Threshold: 2000 events/sec from a single HCP cluster namespace
    # Normal log volume varies by cluster activity, but sustained >2000 events/sec may indicate:
    #   - Runaway logging (operator in crash loop, error logging loops)
    #   - Debug logging left enabled on control plane components
    #   - Cluster degradation (repeated failures generating error logs)
    #   - Heavy API activity (excessive audit logs - may be expected for some clusters)
    #   - Noisy applications deployed to cluster
    # High log volume from a cluster can:
    #   - Fill Vector buffer faster than S3 can write (causes VectorBufferNearCapacity on that node)
    #   - Increase costs (S3 storage, data transfer)
    #   - Make troubleshooting harder (signal-to-noise ratio)
    # Note: Audit logs (container_name="audit-logs") are typically the highest volume contributor
    #   - Query audit logs specifically with: container_name="audit-logs" label filter
    - alert: VectorClusterExcessiveLogsSRE
      expr: |
        sum by (pod_namespace) (
          rate(vector_component_received_events_total{component_id="kubernetes_logs"}[5m])
        ) > 2000
      for: 15m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "HCP cluster {{ $labels.pod_namespace }} generating excessive logs"
        description: "Log rate: {{ $value | humanize }} events/sec (>2000 threshold). This may be normal during high activity or indicate logging issues."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorClusterExcessiveLogsSRE.md

    # WARNING: Specific HCP cluster has poor pipeline processing rate
    # One cluster's logs are being dropped or filtered in the transform pipeline
    # NOTE: This measures transform pipeline health, not S3 delivery success (custom metric created before S3 sink)
    # May indicate VRL errors in transforms, invalid log formats, or cluster-specific parsing issues
    - alert: VectorClusterPipelineIssuesSRE
      expr: |
        vector:log_delivery:pipeline_ratio:by_cluster < 0.95
      for: 30m
      labels:
        severity: warning
        namespace: openshift-monitoring
      annotations:
        summary: "HCP cluster {{ $labels.pod_namespace }} has pipeline processing issues"
        description: "Pipeline throughput: {{ $value | humanizePercentage }} (<95% threshold). Logs may be dropped in transforms. Check for VRL errors in enrich_metadata, parse_json_logs, or parse_plain_text_timestamps."
        SOP: https://github.com/openshift/ops-sop/blob/master/v4/alerts/hypershift/VectorClusterPipelineIssuesSRE.md
