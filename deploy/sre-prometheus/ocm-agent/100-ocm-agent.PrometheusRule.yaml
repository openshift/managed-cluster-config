apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-managed-notification-alerts
    role: alert-rules 
  name: sre-managed-notification-alerts
  namespace: openshift-monitoring
spec:
  groups:
  - name: sre-managed-notification-alerts
    rules:
    - alert: LoggingVolumeFillingUpNotificationSRE
    # KubePersistentVolumeFillingUp alert firing in openshift-logging Namespace.
      expr: count(ALERTS{alertname="KubePersistentVolumeFillingUp", alertstate="firing", namespace="openshift-logging"}) >= 1
      for: 30m
      labels:
        severity: Info
        namespace: openshift-logging
        send_managed_notification: "true"
        managed_notification_template: "LoggingVolumeFillingUp"
    - alert: MultipleDefaultStorageClassesNotificationSRE
    # MultipleDefaultStorageClasses alert firing in openshift-logging Namespace.
      expr: count(ALERTS{alertname="MultipleDefaultStorageClasses", alertstate="firing", namespace="openshift-cluster-storage-operator"}) >= 1
      for: 30m
      labels:
        severity: Info
        namespace: openshift-cluster-storage-operator
        send_managed_notification: "true"
        managed_notification_template: "MultipleDefaultStorageClasses"
    - alert: NonSystemChangeValidatingWebhookConfigurationsNotificationSRE
    # splunkforwarder_audit_filter_exposed_splunk_event_total is a counter that starts at 1. 
    # We cannot use increase(), as there is no increase from 0 to 1. 
    # See https://github.com/prometheus/prometheus/issues/1673
    # Therefore, we use the difference between two 5 minute timeslots (and fill up with 0 in case there is no metric)
    # to check if the value increased
      expr: sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m]) or vector(0)) - sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m] offset 5m) or vector(0)) > 0
      labels:
        severity: Info
        namespace: audit-exporter
        send_managed_notification: "true"
        managed_notification_template: "NonSystemChangeValidationWebhookConfigurations"
    - alert: WorkerNodeFileDescriptorLimitSRE
      # This is the same as the upstream alert, but groups by instance id (which is which node is affected)
      # and then only fires on the worker nodes. The `unless` portion is because when getting the node-role
      # metric it splits on the roles, so infra,worker then gives two results, one with just the infra role
      # and one with just the worker role.  So we get just the infra/controlplane nodes and then exclude
      # them from the worker node result, which gives us just the customer worker nodes.
      expr: |-
        group by (instance) (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} >= 90
        )
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 15m
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFileDescriptorAtLimit
        send_managed_notification: "true"
      annotations:
        message: "Kernel is predicted to exhaust file descriptors limit soon."
    - alert: WorkerNodeFilesystemSpaceFillingUp
      # This is the same as the upstream alert, but groups by the whether it's a worker node or notification
      expr: |-
        (node_filesystem_files_free{fstype!="",job="node-exporter"} / node_filesystem_files{fstype!="",job="node-exporter"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!="",job="node-exporter"} == 0)
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 1h
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFilesystemSpaceFillingUp
        send_managed_notification: "true"
      annotations:
        message: "Filesystem is predicted to run out of inodes within the next 4 hours."
    - alert: CustomerWorkloadPreventingDrainSRE
      # This alert will fire when a node is unscheduleable and is failing to drain for an extended period of time based on a customer workload that's failing to drain
      # This alert will NOT fire if there is an ongoing upgrade, or if the node is not deleting.
      expr: |-
        (
          (
            kube_node_spec_unschedulable > 0
            unless
            ignoring(node,container,endpoint,job,namespace,service)
            sum(mapi_machine_set_status_replicas{name=~".*upgrade$"}) > 0
          )
          * on (node)
          group_right ()group by (node)
            (pods_preventing_node_drain)
        )
        * on (node)
        group_left ()group by (node)
          (kube_node_role{role!~"infra|control-plane|master"})
        unless group by(node)
          (kube_node_role{role=~"infra|control-plane|master"})
      for: 30m # KubeNodeUnscheduleableSRE fires at 1h, but since this is specific to customer workloads we can shorten the feedback loop here.
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: CustomerWorkloadPreventingDrain
        send_managed_notification: "true"
      annotations:
        message: "A non-openshift workload is preventing a node from draining."
    - alert: WorkerNodeFilesystemAlmostOutOfFiles
      annotations:
        message: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available inodes left.
        summary: Filesystem has less than 3% inodes left.
      # This is the same as the upstream alert, but groups by node type: worker node or non-worker node
      # Original source: https://github.com/helm/charts/blob/master/stable/prometheus-operator/templates/prometheus/rules-1.14/node-exporter.yaml#L133C1-L146C27
      expr: |-
        (
          node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
        ) * on (instance) group_left () group by (instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        ) unless group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 1h
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFilesystemAlmostOutOfFiles
        send_managed_notification: "true"
