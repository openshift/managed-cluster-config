apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-managed-notification-alerts
    role: alert-rules 
  name: sre-managed-notification-alerts
  namespace: openshift-monitoring
spec:
  groups:
  - name: sre-managed-notification-alerts
    rules:
    - alert: LoggingVolumeFillingUpNotificationSRE1m
    # KubePersistentVolumeFillingUp alert firing in openshift-logging Namespace.
    # See https://github.com/openshift/cluster-monitoring-operator/blob/master/assets/control-plane/prometheus-rule.yaml#L299
      expr: |
        (
          kubelet_volume_stats_available_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_used_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"} > 0
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace="openshift-logging", access_mode="ReadOnlyMany"} == 1
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace="openshift-logging",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: Info
        namespace: openshift-logging
        send_managed_notification: "true"
        managed_notification_template: "LoggingVolumeFillingUp"
    - alert: LoggingVolumeFillingUpNotificationSRE1h
    # KubePersistentVolumeFillingUp alert firing in openshift-logging Namespace.
    # See https://github.com/openshift/cluster-monitoring-operator/blob/b836b1929584f47862cf0693b3b2d6299542cdee/assets/control-plane/prometheus-rule.yaml#L317
      expr: |
       (
          kubelet_volume_stats_available_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_used_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_available_bytes{namespace="openshift-logging",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace="openshift-logging", access_mode="ReadOnlyMany"} == 1
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace="openshift-logging",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1h
      labels:
        severity: Info
        namespace: openshift-logging
        send_managed_notification: "true"
        managed_notification_template: "LoggingVolumeFillingUp"
    - alert: MultipleDefaultStorageClassesNotificationSRE
    # MultipleDefaultStorageClasses alert firing in openshift-logging Namespace.
      expr: count(ALERTS{alertname="MultipleDefaultStorageClasses", alertstate="firing", namespace="openshift-cluster-storage-operator"}) >= 1
      for: 30m
      labels:
        severity: Info
        namespace: openshift-cluster-storage-operator
        send_managed_notification: "true"
        managed_notification_template: "MultipleDefaultStorageClasses"
    - alert: NonSystemChangeValidatingWebhookConfigurationsNotificationSRE
    # splunkforwarder_audit_filter_exposed_splunk_event_total is a counter that starts at 1. 
    # We cannot use increase(), as there is no increase from 0 to 1. 
    # See https://github.com/prometheus/prometheus/issues/1673
    # Therefore, we use the difference between two 5 minute timeslots (and fill up with 0 in case there is no metric)
    # to check if the value increased
      expr: sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m]) or vector(0)) - sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m] offset 5m) or vector(0)) > 0
      labels:
        severity: Info
        namespace: audit-exporter
        send_managed_notification: "true"
        managed_notification_template: "NonSystemChangeValidationWebhookConfigurations"
    - alert: WorkerNodeFileDescriptorLimitSRE
      # This is the same as the upstream alert, but groups by instance id (which is which node is affected)
      # and then only fires on the worker nodes. The `unless` portion is because when getting the node-role
      # metric it splits on the roles, so infra,worker then gives two results, one with just the infra role
      # and one with just the worker role.  So we get just the infra/controlplane nodes and then exclude
      # them from the worker node result, which gives us just the customer worker nodes.
      expr: |-
        group by (instance) (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} >= 90
        )
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 15m
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFileDescriptorAtLimit
        send_managed_notification: "true"
      annotations:
        message: "Kernel is predicted to exhaust file descriptors limit soon."
    - alert: WorkerNodeFilesystemSpaceFillingUp
      # This is the same as the upstream alert, but groups by the whether it's a worker node or notification
      expr: |-
        (node_filesystem_files_free{fstype!="",job="node-exporter"} / node_filesystem_files{fstype!="",job="node-exporter"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!="",job="node-exporter"} == 0)
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 1h
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFilesystemSpaceFillingUp
        send_managed_notification: "true"
      annotations:
        message: "Filesystem is predicted to run out of inodes within the next 4 hours."
