apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: sre-managed-notification-alerts
    role: alert-rules 
  name: sre-managed-notification-alerts
  namespace: openshift-monitoring
spec:
  groups:
  - name: sre-managed-notification-alerts
    rules:
    - alert: LoggingVolumeFillingUpNotificationSRE
    # KubePersistentVolumeFillingUp alert firing in openshift-logging Namespace.
      expr: count(ALERTS{alertname="KubePersistentVolumeFillingUp", alertstate="firing", namespace="openshift-logging"}) >= 1
      for: 30m
      labels:
        severity: Info
        namespace: openshift-logging
        send_managed_notification: "true"
        managed_notification_template: "LoggingVolumeFillingUp"
    - alert: MultipleDefaultStorageClassesNotificationSRE
    # MultipleDefaultStorageClasses alert firing in openshift-logging Namespace.
      expr: count(ALERTS{alertname="MultipleDefaultStorageClasses", alertstate="firing", namespace="openshift-cluster-storage-operator"}) >= 1
      for: 30m
      labels:
        severity: Info
        namespace: openshift-cluster-storage-operator
        send_managed_notification: "true"
        managed_notification_template: "MultipleDefaultStorageClasses"
    - alert: NonSystemChangeValidatingWebhookConfigurationsNotificationSRE
    # splunkforwarder_audit_filter_exposed_splunk_event_total is a counter that starts at 1. 
    # We cannot use increase(), as there is no increase from 0 to 1. 
    # See https://github.com/prometheus/prometheus/issues/1673
    # Therefore, we use the difference between two 5 minute timeslots (and fill up with 0 in case there is no metric)
    # to check if the value increased
      expr: sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m]) or vector(0)) - sum(max_over_time(splunkforwarder_audit_filter_exposed_splunk_event_total{alert="NonSystemChangeValidationWebhookConfiguration"}[5m] offset 5m) or vector(0)) > 0
      labels:
        severity: Info
        namespace: audit-exporter
        send_managed_notification: "true"
        managed_notification_template: "NonSystemChangeValidationWebhookConfigurations"
    - alert: WorkerNodeFileDescriptorLimitSRE
      # This is the same as the upstream alert, but groups by instance id (which is which node is affected)
      # and then only fires on the worker nodes. The `unless` portion is because when getting the node-role
      # metric it splits on the roles, so infra,worker then gives two results, one with just the infra role
      # and one with just the worker role.  So we get just the infra/controlplane nodes and then exclude
      # them from the worker node result, which gives us just the customer worker nodes.
      expr: |-
        group by (instance) (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} >= 90
        )
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 15m
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFileDescriptorAtLimit
        send_managed_notification: "true"
      annotations:
        message: "Kernel is predicted to exhaust file descriptors limit soon."
    - alert: WorkerNodeFilesystemSpaceFillingUp
      # This is the same as the upstream alert, but groups by the whether it's a worker node or notification
      expr: |-
        (node_filesystem_files_free{fstype!="",job="node-exporter"} / node_filesystem_files{fstype!="",job="node-exporter"} * 100 < 20 and predict_linear(node_filesystem_files_free{fstype!="",job="node-exporter"}[6h], 4 * 60 * 60) < 0 and node_filesystem_readonly{fstype!="",job="node-exporter"} == 0)
        * on (instance) group_left ()group by(instance) (
          label_replace(kube_node_role{role!~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
        unless
        group by(instance) (
          label_replace(kube_node_role{role=~"infra|control-plane|master"}, "instance", "$1", "node", "(.*)")
        )
      for: 1h
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: WorkerNodeFilesystemSpaceFillingUp
        send_managed_notification: "true"
      annotations:
        message: "Filesystem is predicted to run out of inodes within the next 4 hours."
    - alert: CustomerWorkloadPreventingDrainSRE
      # This alert will fire when a node is unscheduleable and is failing to drain for an extended period of time based on a customer workload that's failing to drain
      # This alert will NOT fire if there is an ongoing upgrade, or if the node is not deleting.
      expr: |-
        (
          (
            kube_node_spec_unschedulable > 0
            unless
            ignoring(node,container,endpoint,job,namespace,service)
            sum(mapi_machine_set_status_replicas{name=~".*upgrade$"}) > 0
          )
          * on (node)
          group_right ()group by (node)
            (pods_preventing_node_drain)
        )
        * on (node)
        group_left ()group by (node)
          (kube_node_role{role!~"infra|control-plane|master"})
        unless group by(node)
          (kube_node_role{role=~"infra|control-plane|master"})
      for: 30m # KubeNodeUnscheduleableSRE fires at 1h, but since this is specific to customer workloads we can shorten the feedback loop here.
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: CustomerWorkloadPreventingDrain
        send_managed_notification: "true"
      annotations:
        message: "A non-openshift workload is preventing a node from draining."
    - alert: UWMPersistentVolumeFillingUpSRE
      # This alert will fire when a kube PV is filling up in the user workload monitoring namespace
      expr: |-
        (
          kubelet_volume_stats_available_bytes{namespace="openshift-user-workload-monitoring",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"openshift-user-workload-monitoring",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_used_bytes{namespace=~"openshift-user-workload-monitoring",job="kubelet", metrics_path="/metrics"} > 0
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"openshift-user-workload-monitoring", access_mode="ReadOnlyMany"} == 1
        unless on(namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"openshift-user-workload-monitoring",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: critical
        namespace: openshift-monitoring
        managed_notification_template: UWMPersistentVolumeFillingUp
        send_managed_notification: "true"
      annotations:
        message: "A persistent volume for User Workload Monitoring is filling up."

