apiVersion: v1
kind: ConfigMap
metadata:
  name: vector-config
  namespace: hypershift-control-plane-log-forwarding
data:
  vector.yaml: |
    # Vector configuration for multi-tenant logging
    data_dir: "/vector-data-dir"
    
    api:
      enabled: true
      address: "0.0.0.0:8686"
      playground: false
    
    # Sources
    sources:
      kubernetes_logs:
        type: "kubernetes_logs"
        # Collect logs from all pods on the node
        extra_namespace_label_selector: "hypershift.openshift.io/hosted-control-plane=true"
        glob_minimum_cooldown_ms: 1000
        auto_partial_merge: true
        use_apiserver_cache: true
        namespace_annotation_fields:
          - metadata.labels

      vector_metrics:
        type: internal_metrics
    
    # Transforms
    transforms:  
      enrich_metadata:
        type: "remap"
        inputs: ["kubernetes_logs"]
        source: |
          # Extract metadata for S3 path structure
          .mc_cluster_id = get_env_var("MC_CLUSTER_ID") ?? "unknown"
          .namespace = .kubernetes.pod_namespace || "unknown"
          .application = .kubernetes.pod_labels.app || "unknown"
          .pod_name = .kubernetes.pod_name || "unknown"
          .container_name = .kubernetes.container_name || "container"
          
          # Keep timestamp
          if !exists(.timestamp) {
            .timestamp = now()
          }
          
          # Store Vector's ingestion timestamp before any processing
          .ingest_timestamp = .timestamp
          
      parse_json_logs:
        type: "remap"
        inputs: ["enrich_metadata"]
        source: |
          # Attempt to parse the message field as JSON for timestamp extraction only
          # Do NOT merge parsed JSON into the top-level event - preserve original message
          # Original JSON payload should remain in .message for delivery

          # Performance optimization: only attempt JSON parsing if message looks like JSON
          # CHANGED: Use fallible operator to preserve events without message field
          message_str, str_err = string(.message)
          if str_err != null {
            # Message field is missing or not a string - preserve the event but skip JSON parsing
            if !exists(.message) {
              .message = "[NO_MESSAGE_FIELD]"
            }
            # Keep Vector's ingestion timestamp, don't attempt JSON parsing
          } else if starts_with(message_str, "{") || starts_with(message_str, "[") {
            parsed, err = parse_json(.message)
            if err == null && is_object(parsed) {
              # Handle timestamp precedence: prefer JSON log timestamps over Vector's timestamp
              # Priority order: ts -> time -> stageTimestamp -> requestReceivedTimestamp -> Vector default

              # Try 'ts' field first (etcd, ignition-server style)
              if exists(parsed.ts) {
                ts_value = parsed.ts
                if is_string(ts_value) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(ts_value, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  } else {
                    # Try Unix timestamp in string format
                    unix_ts, unix_err = to_float(ts_value)
                    if unix_err == null {
                      # Convert to timestamp (handle both seconds and milliseconds)
                      # CHANGED: Use fallible operators for conversions
                      unix_ts_int, int_err = to_int(unix_ts)
                      if int_err == null {
                        if unix_ts < 1000000000000.0 {
                          ts_result, ts_conv_err = from_unix_timestamp(unix_ts_int, "seconds")
                          if ts_conv_err == null {
                            .timestamp = ts_result
                          }
                        } else {
                          unix_ts_ms = unix_ts / 1000
                          unix_ts_ms_int, ms_int_err = to_int(unix_ts_ms)
                          if ms_int_err == null {
                            ts_result, ts_conv_err = from_unix_timestamp(unix_ts_ms_int, "seconds")
                            if ts_conv_err == null {
                              .timestamp = ts_result
                            }
                          }
                        }
                      }
                    }
                  }
                } else if is_float(ts_value) || is_integer(ts_value) {
                  # Handle numeric timestamps
                  # CHANGED: Use fallible operators for conversions
                  ts_float, float_err = to_float(ts_value)
                  if float_err == null {
                    if ts_float < 1000000000000.0 {
                      ts_int, int_err = to_int(ts_value)
                      if int_err == null {
                        ts_result, ts_conv_err = from_unix_timestamp(ts_int, "seconds")
                        if ts_conv_err == null {
                          .timestamp = ts_result
                        }
                      }
                    } else {
                      ts_seconds = ts_float / 1000.0
                      ts_seconds_int, int_err = to_int(ts_seconds)
                      if int_err == null {
                        ts_result, ts_conv_err = from_unix_timestamp(ts_seconds_int, "seconds")
                        if ts_conv_err == null {
                          .timestamp = ts_result
                        }
                      }
                    }
                  }
                }
              # Try 'time' field as fallback (alternative JSON timestamp field)
              } else if exists(parsed.time) {
                time_value = parsed.time
                if is_string(time_value) {
                  # Handle ISO timestamp strings
                  parsed_time, time_err = parse_timestamp(time_value, "%+")
                  if time_err == null {
                    .timestamp = parsed_time
                  }
                }
              # Try 'stageTimestamp' field (Kubernetes audit logs - primary)
              } else if exists(parsed.stageTimestamp) {
                stage_timestamp = parsed.stageTimestamp
                if is_string(stage_timestamp) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(stage_timestamp, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  }
                }
              # Try 'requestReceivedTimestamp' field (Kubernetes audit logs - fallback)
              } else if exists(parsed.requestReceivedTimestamp) {
                request_timestamp = parsed.requestReceivedTimestamp
                if is_string(request_timestamp) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(request_timestamp, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  }
                }
              }
            }
          }
      
      parse_plain_text_timestamps:
        type: "remap"
        inputs: ["parse_json_logs"]
        source: |
          # Parse timestamps from plain text log messages
          # This handles logs that weren't parsed as JSON or didn't have timestamp fields

          # CHANGED: Use fallible operator to preserve events without string message field
          message, msg_err = string(.message)
          if msg_err != null {
            # Message is not a string - skip timestamp parsing but preserve the event
            # The event will keep its Vector ingestion timestamp or JSON-parsed timestamp from previous stage
            # Exit this transform early without further processing
            # (VRL doesn't have explicit return, so we just skip all parsing logic)
          } else {
            # Priority order for timestamp extraction (try most specific first)

            # 1. Direct ISO timestamp at start: "2025-08-30T06:11:26.816Z Message here"
            iso_match, iso_err = parse_regex(message, r'^(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d{3,6})?Z?)')
            if iso_err == null && iso_match != null {
              parsed_ts, ts_err = parse_timestamp(iso_match.timestamp, "%+")
              if ts_err == null {
                .timestamp = parsed_ts
              }
            } else {
              # 2. time="timestamp" format: 'time="2025-08-30T09:21:21Z" Additional content'
              time_match, time_err = parse_regex(message, r'time="(?P<timestamp>[^"]+)"')
              if time_err == null && time_match != null {
                parsed_ts, ts_err = parse_timestamp(time_match.timestamp, "%+")
                if ts_err == null {
                  .timestamp = parsed_ts
                }
              }
              # 3. Kubernetes log format: "I0830 11:27:01.564974 1 controller.go:231] Message"
              k8s_match, k8s_err = parse_regex(message, r'^[IWEF](?P<month>\d{2})(?P<day>\d{2})\s+(?P<hour>\d{2}):(?P<minute>\d{2}):(?P<second>\d{2})\.(?P<microsecond>\d{6})')
              if k8s_err == null && k8s_match != null {
                # Construct ISO timestamp (assume current year)
                # CHANGED: Use fallible operators for formatting and slicing
                current_year, year_err = format_timestamp(now(), "%Y")
                if year_err == null {
                  # Extract milliseconds (first 3 digits of microseconds)
                  millis, slice_err = slice(k8s_match.microsecond, start: 0, end: 3)
                  if slice_err == null {
                    # Build ISO timestamp using join function
                    parts = [current_year, "-", k8s_match.month, "-", k8s_match.day, "T",
                            k8s_match.hour, ":", k8s_match.minute, ":", k8s_match.second, ".", millis, "Z"]
                    iso_timestamp, join_err = join(parts, "")
                    if join_err == null {
                      parsed_ts, ts_err = parse_timestamp(iso_timestamp, "%+")
                      if ts_err == null {
                        .timestamp = parsed_ts
                      }
                    }
                  }
                }
              # 4. Go standard log format: "2025/08/30 10:33:20 message"
              } else {
                go_match, go_err = parse_regex(message, r'^(?P<year>\d{4})/(?P<month>\d{2})/(?P<day>\d{2})\s+(?P<hour>\d{2}):(?P<minute>\d{2}):(?P<second>\d{2})')
                if go_err == null && go_match != null {
                  # Build ISO timestamp using join function
                  # CHANGED: Use fallible operator for join
                  parts = [go_match.year, "-", go_match.month, "-", go_match.day, "T",
                          go_match.hour, ":", go_match.minute, ":", go_match.second, "Z"]
                  iso_timestamp, join_err = join(parts, "")
                  if join_err == null {
                    parsed_ts, ts_err = parse_timestamp(iso_timestamp, "%+")
                    if ts_err == null {
                      .timestamp = parsed_ts
                    }
                  }
                }
              }
            }
          }

      # Create custom per-cluster metrics for S3 delivery monitoring
      # Converts log events into Prometheus metrics with source pod labels
      cluster_metrics:
        type: "log_to_metric"
        inputs: ["parse_plain_text_timestamps"]
        metrics:
          - type: "counter"
            field: "message"
            name: "hcp_logs_events_total"
            namespace: "vector"
            tags:
              pod_namespace: '{{ "{{" }} namespace {{ "}}" }}'
              pod_name: '{{ "{{" }} pod_name {{ "}}" }}'
              container_name: '{{ "{{" }} container_name {{ "}}" }}'
              mc_cluster_id: '{{ "{{" }} mc_cluster_id {{ "}}" }}'
              application: '{{ "{{" }} application {{ "}}" }}'

    # Sinks
    sinks:
      prometheus:
        type: prometheus_exporter
        inputs:
          - vector_metrics      # Internal Vector component metrics
          - cluster_metrics     # Custom per-cluster metrics with source labels

      hcp_logs:
        type: "aws_s3"
        inputs: ["parse_plain_text_timestamps"]

        # S3 bucket configuration
        bucket: "${S3_BUCKET_NAME}"
        region: "${AWS_REGION}"
        
        # Escape the following string so hive doesn't attempt to interpolate
        # the same syntax as vector does during the selectorsyncset apply
        # The weird brace syntax is how you escape these in the text/template
        # go library
        # Dynamic key prefix based on cluster/namespace/app/pod structure
        key_prefix: '{{ "{{" }} mc_cluster_id {{ "}}" }}/{{ "{{" }} namespace {{ "}}" }}/{{ "{{" }} application {{ "}}" }}/{{ "{{" }} pod_name {{ "}}" }}/{{ "{{" }} container_name {{ "}}" }}/'
        
        # Request settings
        request:
          retry_attempts: 3
          retry_initial_backoff_secs: 1
          retry_max_duration_secs: 30
          timeout_secs: 30
        
        # Compression
        compression: "gzip"
        
        # File format
        encoding:
          codec: "json"
          framing:
            method: "newline_delimited"
        
        # Buffer configuration
        buffer:
          type: "disk"
          max_size: 10737418240  # 10GB
          when_full: "block"
        
        # Authentication via IRSA
        auth:
          assume_role: "${S3_WRITER_ROLE_ARN}"
          
        # Add timestamp to filename
        filename_append_uuid: true
        filename_time_format: "%Y%m%d-%H%M%S"
        filename_extension: "json.gz"
