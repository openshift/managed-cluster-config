apiVersion: v1
kind: ConfigMap
metadata:
  name: vector-config
  namespace: hypershift-control-plane-log-forwarding
data:
  vector.yaml: |
    # Vector configuration for multi-tenant logging
    data_dir: "/vector-data-dir"
    
    api:
      enabled: true
      address: "0.0.0.0:8686"
      playground: false
    
    # Sources
    sources:
      kubernetes_logs:
        type: "kubernetes_logs"
        # Collect logs from all pods on the node
        extra_namespace_label_selector: "hypershift.openshift.io/hosted-control-plane=true"
        glob_minimum_cooldown_ms: 1000
        auto_partial_merge: true
        use_apiserver_cache: true
        namespace_annotation_fields:
          - metadata.labels

      vector_metrics:
        type: internal_metrics
    
    # Transforms
    transforms:  
      enrich_metadata:
        type: "remap"
        inputs: ["kubernetes_logs"]
        source: |
          # Extract metadata for S3 path structure
          .mc_cluster_id = get_env_var!("MC_CLUSTER_ID") || "unknown"
          .namespace = .kubernetes.pod_namespace || "unknown"
          .application = .kubernetes.pod_labels.app || "unknown"
          .pod_name = .kubernetes.pod_name || "unknown"
          .container_name = .kubernetes.container_name || "container"
          
          # Keep timestamp
          if !exists(.timestamp) {
            .timestamp = now()
          }
          
          # Store Vector's ingestion timestamp before any processing
          .ingest_timestamp = .timestamp
          
      parse_json_logs:
        type: "remap"
        inputs: ["enrich_metadata"]
        source: |
          # Attempt to parse the message field as JSON for timestamp extraction only
          # Do NOT merge parsed JSON into the top-level event - preserve original message
          # Original JSON payload should remain in .message for delivery
          
          # Performance optimization: only attempt JSON parsing if message looks like JSON
          message_str = string!(.message)
          if starts_with(message_str, "{") || starts_with(message_str, "[") {
            parsed, err = parse_json(.message)
            if err == null && is_object(parsed) {
              # Handle timestamp precedence: prefer JSON log timestamps over Vector's timestamp
              # Priority order: ts -> time -> stageTimestamp -> requestReceivedTimestamp -> Vector default
              
              # Try 'ts' field first (etcd, ignition-server style)
              if exists(parsed.ts) {
                ts_value = parsed.ts
                if is_string(ts_value) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(ts_value, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  } else {
                    # Try Unix timestamp in string format
                    unix_ts, unix_err = to_float(ts_value)
                    if unix_err == null {
                      # Convert to timestamp (handle both seconds and milliseconds)
                      unix_ts_int = to_int(unix_ts)
                      if unix_ts < 1000000000000.0 {
                        .timestamp = from_unix_timestamp!(unix_ts_int, "seconds")
                      } else {
                        unix_ts_ms = unix_ts / 1000
                        .timestamp = from_unix_timestamp!(to_int(unix_ts_ms), "seconds")
                      }
                    }
                  }
                } else if is_float(ts_value) || is_integer(ts_value) {
                  # Handle numeric timestamps
                  ts_float = to_float!(ts_value)
                  if ts_float < 1000000000000.0 {
                    .timestamp = from_unix_timestamp!(to_int!(ts_value), "seconds")
                  } else {
                    ts_seconds = to_float!(ts_value) / 1000.0
                    .timestamp = from_unix_timestamp!(to_int(ts_seconds), "seconds")
                  }
                }
              # Try 'time' field as fallback (alternative JSON timestamp field)
              } else if exists(parsed.time) {
                time_value = parsed.time
                if is_string(time_value) {
                  # Handle ISO timestamp strings
                  parsed_time, time_err = parse_timestamp(time_value, "%+")
                  if time_err == null {
                    .timestamp = parsed_time
                  }
                }
              # Try 'stageTimestamp' field (Kubernetes audit logs - primary)
              } else if exists(parsed.stageTimestamp) {
                stage_timestamp = parsed.stageTimestamp
                if is_string(stage_timestamp) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(stage_timestamp, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  }
                }
              # Try 'requestReceivedTimestamp' field (Kubernetes audit logs - fallback)
              } else if exists(parsed.requestReceivedTimestamp) {
                request_timestamp = parsed.requestReceivedTimestamp
                if is_string(request_timestamp) {
                  # Handle ISO timestamp strings
                  parsed_ts, ts_err = parse_timestamp(request_timestamp, "%+")
                  if ts_err == null {
                    .timestamp = parsed_ts
                  }
                }
              }
            }
          }
      
      parse_plain_text_timestamps:
        type: "remap"
        inputs: ["parse_json_logs"]
        source: |
          # Parse timestamps from plain text log messages
          # This handles logs that weren't parsed as JSON or didn't have timestamp fields
          
          message = string!(.message)
          
          # Priority order for timestamp extraction (try most specific first)
          
          # 1. Direct ISO timestamp at start: "2025-08-30T06:11:26.816Z Message here"
          iso_match, iso_err = parse_regex(message, r'^(?P<timestamp>\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d{3,6})?Z?)')
          if iso_err == null && iso_match != null {
            parsed_ts, ts_err = parse_timestamp(iso_match.timestamp, "%+")
            if ts_err == null {
              .timestamp = parsed_ts
            }
          } else {
            # 2. time="timestamp" format: 'time="2025-08-30T09:21:21Z" Additional content'
            time_match, time_err = parse_regex(message, r'time="(?P<timestamp>[^"]+)"')
            if time_err == null && time_match != null {
              parsed_ts, ts_err = parse_timestamp(time_match.timestamp, "%+")
              if ts_err == null {
                .timestamp = parsed_ts
              }
            }
            # 3. Kubernetes log format: "I0830 11:27:01.564974 1 controller.go:231] Message"
            k8s_match, k8s_err = parse_regex(message, r'^[IWEF](?P<month>\d{2})(?P<day>\d{2})\s+(?P<hour>\d{2}):(?P<minute>\d{2}):(?P<second>\d{2})\.(?P<microsecond>\d{6})')
            if k8s_err == null && k8s_match != null {
              # Construct ISO timestamp (assume current year)
              current_year = format_timestamp!(now(), "%Y")
              # Extract milliseconds (first 3 digits of microseconds)
              millis = slice!(k8s_match.microsecond, start: 0, end: 3)
              # Build ISO timestamp using join function
              parts = [current_year, "-", k8s_match.month, "-", k8s_match.day, "T", 
                      k8s_match.hour, ":", k8s_match.minute, ":", k8s_match.second, ".", millis, "Z"]
              iso_timestamp = join!(parts, "")
              parsed_ts, ts_err = parse_timestamp(iso_timestamp, "%+")
              if ts_err == null {
                .timestamp = parsed_ts
              }
            # 4. Go standard log format: "2025/08/30 10:33:20 message"
            } else {
              go_match, go_err = parse_regex(message, r'^(?P<year>\d{4})/(?P<month>\d{2})/(?P<day>\d{2})\s+(?P<hour>\d{2}):(?P<minute>\d{2}):(?P<second>\d{2})')
              if go_err == null && go_match != null {
                # Build ISO timestamp using join function
                parts = [go_match.year, "-", go_match.month, "-", go_match.day, "T", 
                        go_match.hour, ":", go_match.minute, ":", go_match.second, "Z"]
                iso_timestamp = join!(parts, "")
                parsed_ts, ts_err = parse_timestamp(iso_timestamp, "%+")
                if ts_err == null {
                  .timestamp = parsed_ts
                }
              }
            }
          }
    
    # Sinks
    sinks:
      prometheus:
        type: prometheus_exporter
        inputs:
          - vector_metrics

      hcp_logs:
        type: "aws_s3"
        inputs: ["parse_plain_text_timestamps"]
        
        # S3 bucket configuration
        bucket: "${S3_BUCKET_NAME}"
        region: "${AWS_REGION}"
        
        # Escape the following string so hive doesn't attempt to interpolate
        # the same syntax as vector does during the selectorsyncset apply
        # The weird brace syntax is how you escape these in the text/template
        # go library
        # Dynamic key prefix based on cluster/namespace/app/pod structure
        key_prefix: '{{ "{{" }} mc_cluster_id {{ "}}" }}/{{ "{{" }} namespace {{ "}}" }}/{{ "{{" }} application {{ "}}" }}/{{ "{{" }} pod_name {{ "}}" }}/{{ "{{" }} container_name {{ "}}" }}/'
        
        # Request settings
        request:
          retry_attempts: 3
          retry_initial_backoff_secs: 1
          retry_max_duration_secs: 30
          timeout_secs: 30
        
        # Compression
        compression: "gzip"
        
        # File format
        encoding:
          codec: "json"
          framing:
            method: "newline_delimited"
        
        # Buffer configuration
        buffer:
          type: "disk"
          max_size: 10737418240  # 10GB
          when_full: "block"
        
        # Authentication via IRSA
        auth:
          assume_role: "${S3_WRITER_ROLE_ARN}"
          
        # Add timestamp to filename
        filename_append_uuid: true
        filename_time_format: "%Y%m%d-%H%M%S"
        filename_extension: "json.gz"
