# taken from f029cca0-fd96-4cde-afa4-77c151c5233d
# using oc get prometheusrule -n openshift-logging collector -oyaml
# also:
# oc get csv -n openshift-logging | grep -i -e elastic -elogging
# cluster-logging.5.3.5-20        Red Hat OpenShift Logging          5.3.5-20 Succeeded
# elasticsearch-operator.5.3.5-20 OpenShift Elasticsearch Operator   5.3.5-20 Succeeded
# and
# https://catalog.redhat.com/software/containers/openshift-logging/cluster-logging-operator-bundle/5fd22f465d2ec16f0da1e8c8
# where the current latest version is:
# ```
# $ date
# Sun May  1 16:04:22 IDT 2022
# $ echo "the version is v5.4.0-138" # there is also 5.4.6-46
# ...
# ```
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector
  namespace: openshift-logging
spec:
  groups:
  - name: logging_fluentd.alerts
    rules:
    - alert: FluentdNodeDown
      annotations:
        message: Prometheus could not scrape fluentd {{ $labels.instance }} for more
          than 10m.
        summary: Fluentd cannot be scraped
      expr: |
        absent(up{job="collector"} == 1)
      for: 10m
      labels:
        service: fluentd
        severity: critical
    - alert: FluentdQueueLengthIncreasing
      annotations:
        message: For the last hour, fluentd {{ $labels.instance }} average buffer
          queue length has increased continuously.
        summary: Fluentd unable to keep up with traffic over time.
      expr: |
        deriv(fluentd_output_status_buffer_queue_length[10m]) > 0 and delta(fluentd_output_status_buffer_queue_length[1h]) > 1
      for: 1h
      labels:
        service: fluentd
        severity: error
    - alert: FluentDHighErrorRate
      annotations:
        message: '{{ $value }}% of records have resulted in an error by fluentd {{
          $labels.instance }}.'
        summary: FluentD output errors are high
      expr: |
        100 * (
          sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
        /
          sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
        ) > 10
      for: 15m
      labels:
        severity: warning
    - alert: FluentDVeryHighErrorRate
      annotations:
        message: '{{ $value }}% of records have resulted in an error by fluentd {{
          $labels.instance }}.'
        summary: FluentD output errors are very high
      expr: |
        100 * (
          sum by(instance)(rate(fluentd_output_status_num_errors[2m]))
        /
          sum by(instance)(rate(fluentd_output_status_emit_records[2m]))
        ) > 25
      for: 15m
      labels:
        severity: critical
